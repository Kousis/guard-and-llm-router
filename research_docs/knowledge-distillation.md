## Knowledge Distillation

#### Distillation Scaling Laws

https://arxiv.org/pdf/2502.08606

#### MINIPLM: Knowledge Distillation for Pre-Training Language Models

https://arxiv.org/pdf/2410.17215

